default:
  image: python:3.9-slim
variables:
  OS_AUTH_TYPE: v3applicationcredential
  OS_AUTH_URL: https://hdf-cloud.fz-juelich.de:5000
  OS_IDENTITY_API_VERSION: 3
  OS_REGION_NAME: "HDFCloud"
  OS_INTERFACE: public
  TEST_IP: 134.94.199.220
  OLD_TEST_NAME: old-airflow-test
  TESTING_NAME: airflow-testing
  TESTING_URL: http://134.94.199.220:7001/home

# before script copied from gitlab docs
before_script: &ssh_setup
  - 'command -v ssh-agent >/dev/null || ( apt-get update -y && apt-get install openssh-client gcc libxslt-dev libffi-dev libssl-dev build-essential python3-dev -y )'
  - eval $(ssh-agent -s)
  - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh

stages:
  - test
  - deploy
  - test-deployment
  - cleanup

test:
  stage: test
  image: 
    name: apache/airflow:2.1.3
    entrypoint: [""]
  before_script:
    - airflow db init
    - pip install -r requirements.txt
    - pip install nose==1.3.7
    - airflow connections add --conn-uri https://b2share-testing.fz-juelich.de/ default_b2share
  script: 
   - ls 
   - pwd
   - cp dags/* /opt/airflow/dags/
   - airflow dags list 
   - airflow connections list
   - airflow dags test testdag 2021-08-18
   - nosetests

full-deploy-test:
  stage: deploy
  environment: Testing
  # when: manual
  only:
    - web
  script:
    - echo "Starting the full testing deployment of airflows example."
    - pip install python-openstackclient
    - OLD_ID=`openstack server show $TESTING_NAME -f value -c id`
    - openstack server set --name $OLD_TEST_NAME $OLD_ID
    - INSTANCE_ID=`openstack server create -f value -c id --prefix IMAGE_ --flavor m4 --image 149a65b5-aeb8-499f-aaa6-ec966bd28dd6 --user-data scripts/cloudinit.yml --security-group ssh --security-group airflows --security-group www --security-group https $TESTING_NAME`
    - while [ "`openstack server show $INSTANCE_ID -c addresses -f value`" = "{}" ]; do sleep 5; done # wait until an address is available to attach the floating ip
    - openstack server add floating ip $INSTANCE_ID $TEST_IP
    - echo "Done"

test-testing_dags:
  cache: {}
  stage: test-deployment 
  # only:
  #   - master
  except:
    - tags
  script:
    - apt update && apt -y install curl
    - echo "This is a simple check if the deploments was successful and dags get executed"
    - sleep 150 # ensure that the docker containers are up and running before testing the airflow installation
    - 'curl --insecure -I -H "Accept: application/json" $TESTING_URL'
    - 'curl -X GET -u airflow:airflow -H "Content-Type: application/json" $TEST_IP:7001/api/v1/dags'
    - 'curl -X GET -u airflow:airflow -H "Content-Type: application/json" $TEST_IP:7001/api/v1/connections'
    - 'curl -X POST -u airflow:airflow -H "Content-Type: application/json" --data {} $TEST_IP:7001/api/v1/dags/testdag/dagRuns'

cleanup-successful-full-deployment:
  # check if there is an old prod or test instance, and delete it if present
  stage: cleanup
  when: on_success
  only:
    - web
  script:
    - echo "This is the cleanup for the full-redeployment of the testing or production servers"
    - echo "if this job is reached, all earlier jobs were successful, and any lingering old instances need to be removed"
    - pip install python-openstackclient
    - openstack server delete $OLD_TEST_NAME && echo "Deleted old testing server." || echo "No old testing server found."