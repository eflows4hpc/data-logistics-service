default:
  image: python:3.9-slim

variables:
  DOCKER_TLS_CERTDIR: ""


# before script copied from gitlab docs
before_script:
  - 'command -v ssh-agent >/dev/null || ( apt-get update -y && apt-get install openssh-client gcc libxslt-dev libffi-dev libssl-dev build-essential python3-dev -y )'

stages:
  - test
  - build
  - publish
  - deploy

test:
  stage: test
  image: 
    name: apache/airflow:2.1.3
    entrypoint: [""]
  before_script:
    - echo "DEBUG:"
    - pip --version
    - airflow db init
    - pip install -r requirements.txt
    - pip install nose==1.3.7
    - airflow connections add --conn-uri https://b2share-testing.fz-juelich.de/ default_b2share
  script: 
   - ls 
   - pwd
   - cp dags/* /opt/airflow/dags/
   - airflow dags list 
   - airflow connections list
   - airflow dags test testdag 2021-08-18
   - nosetests

build-custom-image:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  when: manual
  tags:
    - laptop
  variables:
    IMAGE_COMMIT_TAG: $CI_REGISTRY_IMAGE/eflows-airflow:$CI_COMMIT_SHORT_SHA
    IMAGE_LATEST_TAG: $CI_REGISTRY_IMAGE/eflows-airflow:latest

  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build --no-cache=true --pull -t $IMAGE_COMMIT_TAG -f dockers/eflows-airflow.docker .
    - docker push $IMAGE_COMMIT_TAG
    - docker tag $IMAGE_COMMIT_TAG $IMAGE_LATEST_TAG
    - docker push $IMAGE_LATEST_TAG


deploy-test:
  stage: deploy
  environment: Testing
  when: manual
  variables:
    OS_AUTH_TYPE: v3applicationcredential
    OS_AUTH_URL: https://hdf-cloud.fz-juelich.de:5000
    OS_IDENTITY_API_VERSION: 3
    OS_REGION_NAME: "HDFCloud"
    OS_INTERFACE: public
    AIRFLOW__SECRETS__BACKEND_KWARGS: '{"url" : "https://zam10036.zam.kfa-juelich.de", "user" : "${DATACAT_TESTING_USERNAME}", "password" : "${DATACAT_TESTING_PASSWORD}"}'
    AIRFLOW__SECRETS__BACKEND: datacat_integration.secrets.DatacatSecretsBackend
    FLOATING_IP: 134.94.199.220
  script:
    - echo "Starting the full testing deployment of airflows example."
    - pip install python-openstackclient
    - INSTANCE_ID=`openstack server create -f value -c id --prefix IMAGE_ --flavor m4 --image 149a65b5-aeb8-499f-aaa6-ec966bd28dd6 --user-data scripts/cloudinit.yml --security-group ssh --security-group airflows --security-group www --security-group https airflow-testing`
    - while [ "`openstack server show $INSTANCE_ID -c addresses -f value`" = "{}" ]; do sleep 5; done # wait until an address is available to attach the floating ip
    - openstack server add floating ip $INSTANCE_ID $FLOATING_IP
    - echo "Done"

publishgit-do:
  stage: publish
  only:
    - tags
  tags: [stable]
  script:
    - apt-get update 
    - apt-get install -y git
    - (git remote rm gith) || echo "Not found"
    - (git remote -v | grep gith) || git remote add gith "https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/eflows4hpc/data-logistics-service.git"
    - git remote -v
    - git fetch --unshallow origin
    - git push gith +$CI_COMMIT_REF_NAME:refs/heads/main
